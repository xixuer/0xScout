import requests  # å¯¼å…¥requestsåº“ç”¨äºHTTPè¯·æ±‚
import aiohttp   # å¯¼å…¥aiohttpåº“ç”¨äºå¼‚æ­¥HTTPè¯·æ±‚
import asyncio
import time
from bs4 import BeautifulSoup  # å¯¼å…¥BeautifulSoupåº“ç”¨äºè§£æHTMLå†…å®¹
from datetime import datetime  # å¯¼å…¥datetimeæ¨¡å—ç”¨äºè·å–æ—¥æœŸå’Œæ—¶é—´
import os  # å¯¼å…¥osæ¨¡å—ç”¨äºæ–‡ä»¶å’Œç›®å½•æ“ä½œ
from typing import List, Dict, Any, Optional, Union
import traceback
import json

# å¯¼å…¥ç¼“å­˜ç®¡ç†å™¨å’Œæ—¥å¿—
try:
    from src.utils.cache_manager import CacheManager
    from src.logger import LOG
except ImportError:
    try:
        from utils.cache_manager import CacheManager
        from logger import LOG
    except ImportError:
        import logging
        LOG = logging.getLogger(__name__)
        LOG.error("æ— æ³•å¯¼å…¥CacheManagerï¼Œå°†ä¸ä½¿ç”¨ç¼“å­˜åŠŸèƒ½")
        CacheManager = None

import httpx

# è¿è¡Œæ—¶å¯¼å…¥topic_analyzerï¼Œé¿å…è¿˜æ²¡å®‰è£…ä¾èµ–æ—¶å°±å‡ºé”™
# from src.analyzers.topic_analyzer import HackerNewsTopicAnalyzer

try:
    from src.llm import LLM
    from src.config import Settings
except ImportError:
    LOG.warning("æœªèƒ½å¯¼å…¥LLMæˆ–Settingsæ¨¡å—ï¼ŒAIæ€»ç»“åŠŸèƒ½å°†ä¸å¯ç”¨")

class HackerNewsClient:
    """
    HackerNews APIå®¢æˆ·ç«¯
    ç”¨äºè·å–HackerNewsä¸Šçš„çƒ­é—¨æ–‡ç« ã€è¯„è®ºç­‰ä¿¡æ¯
    """
    
    def __init__(self, use_cache=True, cache_ttl=3600):
        """
        åˆå§‹åŒ–HackerNewså®¢æˆ·ç«¯
        
        Args:
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            cache_ttl: ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰
        """
        self.base_url = "https://hacker-news.firebaseio.com/v0"
        
        # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        self.use_cache = use_cache and CacheManager is not None
        if self.use_cache:
            self.cache = CacheManager(cache_dir="cache/hackernews", default_ttl=cache_ttl)
            LOG.info("å·²å¯ç”¨HackerNews APIç¼“å­˜")
        else:
            self.cache = None
            LOG.info("æœªå¯ç”¨HackerNews APIç¼“å­˜")
    
    def get_top_stories(self, limit=30) -> List[int]:
        """
        è·å–çƒ­é—¨æ–‡ç« IDåˆ—è¡¨
        
        Args:
            limit: è¿”å›çš„æ–‡ç« æ•°é‡ä¸Šé™
            
        Returns:
            æ–‡ç« IDåˆ—è¡¨
        """
        LOG.debug(f"è·å–HackerNewsçƒ­é—¨æ–‡ç« IDåˆ—è¡¨ï¼Œé™åˆ¶ä¸º{limit}æ¡")
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"top_stories_{limit}"
        if self.use_cache:
            cached_data = self.cache.get(cache_key)
            if cached_data is not None:
                return cached_data
        
        url = f"{self.base_url}/topstories.json"
        
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            story_ids = response.json()
            
            # é™åˆ¶è¿”å›æ•°é‡
            if limit and len(story_ids) > limit:
                story_ids = story_ids[:limit]
            
            # ç¼“å­˜ç»“æœ
            if self.use_cache:
                # çƒ­é—¨æ–‡ç« å˜åŒ–è¾ƒå¿«ï¼Œä½¿ç”¨è¾ƒçŸ­çš„ç¼“å­˜æ—¶é—´
                self.cache.set(cache_key, story_ids, ttl=600)  # 10åˆ†é’Ÿ
            
            return story_ids
        except Exception as e:
            LOG.error(f"è·å–HackerNewsçƒ­é—¨æ–‡ç« IDåˆ—è¡¨å¤±è´¥ï¼š{e}")
            return []
    
    async def async_get_top_stories(self, limit=30) -> List[int]:
        """
        å¼‚æ­¥è·å–çƒ­é—¨æ–‡ç« IDåˆ—è¡¨
        
        Args:
            limit: è¿”å›çš„æ–‡ç« æ•°é‡ä¸Šé™
            
        Returns:
            æ–‡ç« IDåˆ—è¡¨
        """
        LOG.debug(f"å¼‚æ­¥è·å–HackerNewsçƒ­é—¨æ–‡ç« IDåˆ—è¡¨ï¼Œé™åˆ¶ä¸º{limit}æ¡")
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"top_stories_{limit}"
        if self.use_cache:
            cached_data = self.cache.get(cache_key)
            if cached_data is not None:
                return cached_data
        
        url = f"{self.base_url}/topstories.json"
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=10) as response:
                    if response.status == 200:
                        story_ids = await response.json()
                        
                        # é™åˆ¶è¿”å›æ•°é‡
                        if limit and len(story_ids) > limit:
                            story_ids = story_ids[:limit]
                        
                        # ç¼“å­˜ç»“æœ
                        if self.use_cache:
                            # çƒ­é—¨æ–‡ç« å˜åŒ–è¾ƒå¿«ï¼Œä½¿ç”¨è¾ƒçŸ­çš„ç¼“å­˜æ—¶é—´
                            self.cache.set(cache_key, story_ids, ttl=600)  # 10åˆ†é’Ÿ
                        
                        return story_ids
                    else:
                        error_text = await response.text()
                        LOG.error(f"å¼‚æ­¥è·å–HackerNewsçƒ­é—¨æ–‡ç« IDåˆ—è¡¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}, å“åº”: {error_text}")
                        return []
        except Exception as e:
            LOG.error(f"å¼‚æ­¥è·å–HackerNewsçƒ­é—¨æ–‡ç« IDåˆ—è¡¨æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            return []
    
    def get_item(self, item_id: int) -> Optional[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šIDçš„é¡¹ç›®ï¼ˆæ–‡ç« ã€è¯„è®ºç­‰ï¼‰è¯¦æƒ…
        
        Args:
            item_id: é¡¹ç›®ID
            
        Returns:
            é¡¹ç›®è¯¦æƒ…å­—å…¸ï¼Œå¦‚æœè·å–å¤±è´¥åˆ™è¿”å›None
        """
        LOG.debug(f"è·å–HackerNewsé¡¹ç›®è¯¦æƒ…ï¼ŒID: {item_id}")
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"item_{item_id}"
        if self.use_cache:
            cached_data = self.cache.get(cache_key)
            if cached_data is not None:
                return cached_data
        
        url = f"{self.base_url}/item/{item_id}.json"
        
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            item_data = response.json()
            
            # ç¼“å­˜ç»“æœ
            if self.use_cache:
                # æ–‡ç« å†…å®¹ç›¸å¯¹ç¨³å®šï¼Œå¯ä»¥ä½¿ç”¨è¾ƒé•¿çš„ç¼“å­˜æ—¶é—´
                self.cache.set(cache_key, item_data)
            
            return item_data
        except Exception as e:
            LOG.error(f"è·å–HackerNewsé¡¹ç›®è¯¦æƒ…å¤±è´¥ï¼ŒID: {item_id}, é”™è¯¯: {e}")
            return None
    
    async def async_get_item(self, item_id: int) -> Optional[Dict[str, Any]]:
        """
        å¼‚æ­¥è·å–æŒ‡å®šIDçš„é¡¹ç›®ï¼ˆæ–‡ç« ã€è¯„è®ºç­‰ï¼‰è¯¦æƒ…
        
        Args:
            item_id: é¡¹ç›®ID
            
        Returns:
            é¡¹ç›®è¯¦æƒ…å­—å…¸ï¼Œå¦‚æœè·å–å¤±è´¥åˆ™è¿”å›None
        """
        LOG.debug(f"å¼‚æ­¥è·å–HackerNewsé¡¹ç›®è¯¦æƒ…ï¼ŒID: {item_id}")
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"item_{item_id}"
        if self.use_cache:
            cached_data = self.cache.get(cache_key)
            if cached_data is not None:
                return cached_data
        
        url = f"{self.base_url}/item/{item_id}.json"
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=10) as response:
                    if response.status == 200:
                        item_data = await response.json()
                        
                        # ç¼“å­˜ç»“æœ
                        if self.use_cache:
                            # æ–‡ç« å†…å®¹ç›¸å¯¹ç¨³å®šï¼Œå¯ä»¥ä½¿ç”¨è¾ƒé•¿çš„ç¼“å­˜æ—¶é—´
                            self.cache.set(cache_key, item_data)
                        
                        return item_data
                    else:
                        error_text = await response.text()
                        LOG.error(f"å¼‚æ­¥è·å–HackerNewsé¡¹ç›®è¯¦æƒ…å¤±è´¥ï¼ŒID: {item_id}, çŠ¶æ€ç : {response.status}, å“åº”: {error_text}")
                        return None
        except Exception as e:
            LOG.error(f"å¼‚æ­¥è·å–HackerNewsé¡¹ç›®è¯¦æƒ…æ—¶å‘ç”Ÿå¼‚å¸¸ï¼ŒID: {item_id}, é”™è¯¯: {e}")
            return None
    
    def get_top_stories_details(self, limit=30) -> List[Dict[str, Any]]:
        """
        è·å–çƒ­é—¨æ–‡ç« è¯¦æƒ…åˆ—è¡¨
        
        Args:
            limit: è¿”å›çš„æ–‡ç« æ•°é‡ä¸Šé™
            
        Returns:
            çƒ­é—¨æ–‡ç« è¯¦æƒ…åˆ—è¡¨
        """
        LOG.debug(f"è·å–HackerNewsçƒ­é—¨æ–‡ç« è¯¦æƒ…ï¼Œé™åˆ¶ä¸º{limit}æ¡")
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"top_stories_details_{limit}"
        if self.use_cache:
            cached_data = self.cache.get(cache_key)
            if cached_data is not None:
                return cached_data
        
        # è·å–çƒ­é—¨æ–‡ç« IDåˆ—è¡¨
        story_ids = self.get_top_stories(limit)
        
        # è·å–æ¯ç¯‡æ–‡ç« çš„è¯¦æƒ…
        stories = []
        for story_id in story_ids:
            story = self.get_item(story_id)
            if story:
                stories.append(story)
        
        # ç¼“å­˜ç»“æœ
        if self.use_cache:
            # æ–‡ç« è¯¦æƒ…å˜åŒ–è¾ƒå¿«ï¼Œä½¿ç”¨è¾ƒçŸ­çš„ç¼“å­˜æ—¶é—´
            self.cache.set(cache_key, stories, ttl=600)  # 10åˆ†é’Ÿ
        
        return stories
    
    def fetch_top_stories(self, limit=30) -> List[Dict[str, Any]]:
        """
        è·å–çƒ­é—¨æ–‡ç« åˆ—è¡¨ (å…¼å®¹æ€§æ–¹æ³•ï¼Œå†…éƒ¨è°ƒç”¨ get_top_stories_details)
        
        Args:
            limit: è¿”å›çš„æ–‡ç« æ•°é‡ä¸Šé™
            
        Returns:
            çƒ­é—¨æ–‡ç« è¯¦æƒ…åˆ—è¡¨
        """
        LOG.debug("å‡†å¤‡è·å–Hacker Newsçš„çƒ­é—¨æ–°é—»ã€‚")
        return self.get_top_stories_details(limit=limit)
    
    async def async_get_top_stories_details(self, limit=30) -> List[Dict[str, Any]]:
        """
        å¼‚æ­¥è·å–çƒ­é—¨æ–‡ç« çš„è¯¦ç»†ä¿¡æ¯
        
        Args:
            limit: è¿”å›çš„æ–‡ç« æ•°é‡ä¸Šé™
            
        Returns:
            æ–‡ç« è¯¦æƒ…åˆ—è¡¨
        """
        LOG.debug(f"å¼‚æ­¥è·å–HackerNewsçƒ­é—¨æ–‡ç« è¯¦æƒ…ï¼Œé™åˆ¶ä¸º{limit}æ¡")
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"top_stories_details_{limit}"
        if self.use_cache:
            cached_data = self.cache.get(cache_key)
            if cached_data is not None:
                return cached_data
        
        # è·å–çƒ­é—¨æ–‡ç« IDåˆ—è¡¨
        story_ids = await self.async_get_top_stories(limit)
        
        # å¼‚æ­¥è·å–æ¯ç¯‡æ–‡ç« çš„è¯¦æƒ…
        tasks = []
        for story_id in story_ids:
            task = self.async_get_item(story_id)
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks)
        
        # è¿‡æ»¤æ‰å¤±è´¥çš„ç»“æœ
        stories = [story for story in results if story]
        
        # ç¼“å­˜ç»“æœ
        if self.use_cache:
            # æ–‡ç« è¯¦æƒ…å˜åŒ–è¾ƒå¿«ï¼Œä½¿ç”¨è¾ƒçŸ­çš„ç¼“å­˜æ—¶é—´
            self.cache.set(cache_key, stories, ttl=600)  # 10åˆ†é’Ÿ
        
        return stories
    
    def get_comments(self, story_id: int, limit=30) -> List[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šæ–‡ç« çš„è¯„è®º
        
        Args:
            story_id: æ–‡ç« ID
            limit: è¿”å›çš„è¯„è®ºæ•°é‡ä¸Šé™
            
        Returns:
            è¯„è®ºåˆ—è¡¨
        """
        LOG.debug(f"è·å–HackerNewsæ–‡ç« è¯„è®ºï¼Œæ–‡ç« ID: {story_id}ï¼Œé™åˆ¶ä¸º{limit}æ¡")
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"comments_{story_id}_{limit}"
        if self.use_cache:
            cached_data = self.cache.get(cache_key)
            if cached_data is not None:
                return cached_data
        
        # è·å–æ–‡ç« è¯¦æƒ…
        story = self.get_item(story_id)
        if not story or 'kids' not in story:
            LOG.warning(f"æ–‡ç« ä¸å­˜åœ¨æˆ–æ²¡æœ‰è¯„è®ºï¼Œæ–‡ç« ID: {story_id}")
            return []
        
        # è·å–è¯„è®ºIDåˆ—è¡¨
        comment_ids = story['kids']
        if limit and len(comment_ids) > limit:
            comment_ids = comment_ids[:limit]
        
        # è·å–æ¯æ¡è¯„è®ºçš„è¯¦æƒ…
        comments = []
        for comment_id in comment_ids:
            comment = self.get_item(comment_id)
            if comment and not comment.get('deleted', False) and not comment.get('dead', False):
                comments.append(comment)
        
        # ç¼“å­˜ç»“æœ
        if self.use_cache:
            self.cache.set(cache_key, comments)
        
        return comments
    
    async def async_get_comments(self, story_id: int, limit=30) -> List[Dict[str, Any]]:
        """
        å¼‚æ­¥è·å–æŒ‡å®šæ–‡ç« çš„è¯„è®º
        
        Args:
            story_id: æ–‡ç« ID
            limit: è¿”å›çš„è¯„è®ºæ•°é‡ä¸Šé™
            
        Returns:
            è¯„è®ºåˆ—è¡¨
        """
        LOG.debug(f"å¼‚æ­¥è·å–HackerNewsæ–‡ç« è¯„è®ºï¼Œæ–‡ç« ID: {story_id}ï¼Œé™åˆ¶ä¸º{limit}æ¡")
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"comments_{story_id}_{limit}"
        if self.use_cache:
            cached_data = self.cache.get(cache_key)
            if cached_data is not None:
                return cached_data
        
        # è·å–æ–‡ç« è¯¦æƒ…
        story = await self.async_get_item(story_id)
        if not story or 'kids' not in story:
            LOG.warning(f"æ–‡ç« ä¸å­˜åœ¨æˆ–æ²¡æœ‰è¯„è®ºï¼Œæ–‡ç« ID: {story_id}")
            return []
        
        # è·å–è¯„è®ºIDåˆ—è¡¨
        comment_ids = story['kids']
        if limit and len(comment_ids) > limit:
            comment_ids = comment_ids[:limit]
        
        # å¼‚æ­¥è·å–æ¯æ¡è¯„è®ºçš„è¯¦æƒ…
        tasks = []
        for comment_id in comment_ids:
            task = self.async_get_item(comment_id)
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks)
        
        # è¿‡æ»¤æ‰å·²åˆ é™¤æˆ–å·²æ­»äº¡çš„è¯„è®º
        comments = [comment for comment in results if comment and not comment.get('deleted', False) and not comment.get('dead', False)]
        
        # ç¼“å­˜ç»“æœ
        if self.use_cache:
            self.cache.set(cache_key, comments)
        
        return comments

    def export_top_stories(self, date=None, hour=None, enable_ai_summary=True):
        """
        å¯¼å‡ºå½“å‰ Hacker News çƒ­é—¨æ•…äº‹åˆ—è¡¨åˆ° Markdown æ–‡ä»¶
        
        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD æ ¼å¼)ï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨å½“å‰æ—¥æœŸ
            hour: å°æ—¶å­—ç¬¦ä¸² (HH æ ¼å¼)ï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨å½“å‰å°æ—¶
            enable_ai_summary: æ˜¯å¦å¯ç”¨AIæ‘˜è¦åŠŸèƒ½ï¼Œé»˜è®¤ä¸ºTrue
            
        Returns:
            ç”Ÿæˆçš„ Markdown æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœå‘ç”Ÿé”™è¯¯åˆ™è¿”å› None
        """
        LOG.debug("å‡†å¤‡å¯¼å‡ºHacker Newsçš„çƒ­é—¨æ–°é—»ã€‚")
        
        try:
            # è·å–æ–°é—»æ•°æ®
            stories_details = self.get_top_stories_details(limit=30)  
            
            if not stories_details:
                LOG.warning("æœªæ‰¾åˆ°ä»»ä½•Hacker Newsçš„æ–°é—»ã€‚")
                return None
            
            # å¦‚æœæœªæä¾› date å’Œ hour å‚æ•°ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸå’Œæ—¶é—´
            if date is None:
                date = datetime.now().strftime('%Y-%m-%d')
            if hour is None:
                hour = datetime.now().strftime('%H')

            # æ„å»ºå­˜å‚¨è·¯å¾„
            dir_path = os.path.join('hacker_news', date)
            os.makedirs(dir_path, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
            
            # å¯¹æ•…äº‹è¿›è¡Œåˆ†ç±»
            categorized_stories = self._categorize_stories(stories_details)
            
            # å°è¯•ä¸ºçƒ­é—¨æ•…äº‹æ·»åŠ AIæ‘˜è¦
            if enable_ai_summary:
                LOG.info("AIæ‘˜è¦åŠŸèƒ½å·²å¯ç”¨ï¼Œæ­£åœ¨ä¸ºçƒ­é—¨æ–‡ç« ç”Ÿæˆæ‘˜è¦...")
                top_stories = sorted(stories_details, key=lambda x: x.get('score', 0), reverse=True)[:10]
                self._add_ai_summaries(top_stories)
            else:
                LOG.info("AIæ‘˜è¦åŠŸèƒ½å·²ç¦ç”¨ï¼Œè·³è¿‡æ‘˜è¦ç”Ÿæˆ")
            
            # ä¿å­˜åŸå§‹åˆ—è¡¨
            stories_file_path = os.path.join(dir_path, f'{hour}.md')  # å®šä¹‰æ–‡ä»¶è·¯å¾„
            with open(stories_file_path, 'w', encoding='utf-8') as file:
                file.write(f"# Hacker News çƒ­é—¨æ–°é—» ({date} {hour}:00)\n\n")
                file.write("æœ¬æŠ¥å‘Šæ”¶é›†äº†æœ€è¿‘ä¸€å°æ—¶å†…Hacker Newså¹³å°ä¸Šçš„çƒ­é—¨è®¨è®ºå†…å®¹ã€‚\"åˆ†æ•°\"ä»£è¡¨ç”¨æˆ·çš„æŠ•ç¥¨æ•°é‡ï¼Œåæ˜ äº†æ–‡ç« çš„çƒ­åº¦ä¸å—æ¬¢è¿ç¨‹åº¦ã€‚\n\n")
                
                # é¦–å…ˆæ˜¾ç¤ºæœ€çƒ­é—¨çš„æ•…äº‹
                file.write("## ğŸ”¥ æœ€çƒ­é—¨è®¨è®º\n\n")
                # æŒ‰åˆ†æ•°æ’åºï¼Œå–å‰3ä¸ª
                top_stories = sorted(stories_details, key=lambda x: x.get('score', 0), reverse=True)[:3]
                for idx, story in enumerate(top_stories, start=1):
                    self._write_story_entry(file, idx, story)
                
                # æ·»åŠ å±•ç¤ºç±»å†…å®¹
                if categorized_stories['show_hn']:
                    file.write("\n## ğŸ‘€ å±•ç¤ºä¸åˆ†äº«\n\n")
                    file.write("ç”¨æˆ·åˆ†äº«çš„é¡¹ç›®ã€äº§å“å’Œåˆ›é€ ã€‚\n\n")
                    for idx, story in enumerate(categorized_stories['show_hn'], start=1):
                        self._write_story_entry(file, idx, story)
                
                # æ·»åŠ é—®ç­”ç±»å†…å®¹
                if categorized_stories['ask_hn']:
                    file.write("\n## â“ é—®ç­”ä¸è®¨è®º\n\n")
                    file.write("ç¤¾åŒºæˆå‘˜æå‡ºçš„é—®é¢˜å’Œè®¨è®ºã€‚\n\n")
                    for idx, story in enumerate(categorized_stories['ask_hn'], start=1):
                        self._write_story_entry(file, idx, story)
                
                # æ·»åŠ æŠ€æœ¯ç±»å†…å®¹
                if categorized_stories['tech']:
                    file.write("\n## ğŸ’» æŠ€æœ¯ä¸å·¥ç¨‹\n\n")
                    file.write("ç¼–ç¨‹ã€å¼€å‘å’ŒæŠ€æœ¯ç›¸å…³çš„çƒ­é—¨è¯é¢˜ã€‚\n\n")
                    for idx, story in enumerate(categorized_stories['tech'], start=1):
                        self._write_story_entry(file, idx, story)
                
                # æ·»åŠ ç§‘å­¦ç±»å†…å®¹
                if categorized_stories['science']:
                    file.write("\n## ğŸ”¬ ç§‘å­¦ç ”ç©¶\n\n")
                    file.write("ç§‘å­¦å‘ç°ã€ç ”ç©¶å’Œç›¸å…³è®¨è®ºã€‚\n\n")
                    for idx, story in enumerate(categorized_stories['science'], start=1):
                        self._write_story_entry(file, idx, story)
                
                # æ·»åŠ å•†ä¸šç±»å†…å®¹
                if categorized_stories['business']:
                    file.write("\n## ğŸ’¼ å•†ä¸šä¸åˆ›ä¸š\n\n")
                    file.write("å•†ä¸šæ–°é—»ã€åˆ›ä¸šå’Œè¡Œä¸šåŠ¨æ€ã€‚\n\n")
                    for idx, story in enumerate(categorized_stories['business'], start=1):
                        self._write_story_entry(file, idx, story)
                
                # æ·»åŠ å…¶ä»–ç±»å†…å®¹
                if categorized_stories['others']:
                    file.write("\n## ğŸ“š å…¶ä»–çƒ­é—¨å†…å®¹\n\n")
                    for idx, story in enumerate(categorized_stories['others'], start=1):
                        self._write_story_entry(file, idx, story)
            
            # æ‰§è¡Œä¸»é¢˜åˆ†æ
            topics_file_path = self._analyze_topics(stories_details, date, hour)
            
            LOG.info(f"Hacker Newsçƒ­é—¨æ–°é—»æ–‡ä»¶ç”Ÿæˆï¼š{stories_file_path}")
            return stories_file_path
            
        except Exception as e:
            LOG.error(f"å¯¼å‡ºHacker Newsçƒ­é—¨æ–°é—»æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            LOG.error(traceback.format_exc())
            return None
    
    def _categorize_stories(self, stories):
        """
        å¯¹Hacker Newsæ•…äº‹è¿›è¡Œåˆ†ç±»
        
        Args:
            stories: æ•…äº‹åˆ—è¡¨
            
        Returns:
            åˆ†ç±»åçš„æ•…äº‹å­—å…¸
        """
        categories = {
            'show_hn': [],
            'ask_hn': [],
            'tech': [],
            'science': [],
            'business': [],
            'others': []
        }
        
        # æŠ€æœ¯ç›¸å…³å…³é”®è¯
        tech_keywords = ['programming', 'code', 'software', 'developer', 'web', 'app', 
                         'python', 'javascript', 'rust', 'go', 'java', 'c++', 'typescript',
                         'database', 'api', 'framework', 'library', 'algorithm', 'github',
                         'git', 'docker', 'kubernetes', 'aws', 'cloud', 'devops', 'ai',
                         'ml', 'machine learning', 'deep learning', 'neural', 'llm']
        
        # ç§‘å­¦ç›¸å…³å…³é”®è¯
        science_keywords = ['science', 'research', 'study', 'physics', 'chemistry', 'biology',
                           'astronomy', 'space', 'mars', 'nasa', 'quantum', 'medicine',
                           'climate', 'energy', 'environment', 'genome', 'brain']
        
        # å•†ä¸šç›¸å…³å…³é”®è¯
        business_keywords = ['startup', 'business', 'company', 'founder', 'investor',
                            'venture', 'vc', 'funding', 'acquisition', 'market',
                            'finance', 'economy', 'stock', 'crypto', 'blockchain']
        
        for story in stories:
            title = story.get('title', '').lower()
            
            if story.get('type') == 'job' or 'hiring' in title or 'job' in title:
                categories['business'].append(story)
            elif 'show hn' in title or 'show hn:' in title:
                categories['show_hn'].append(story)
            elif 'ask hn' in title or 'ask hn:' in title:
                categories['ask_hn'].append(story)
            elif any(keyword in title for keyword in tech_keywords):
                categories['tech'].append(story)
            elif any(keyword in title for keyword in science_keywords):
                categories['science'].append(story)
            elif any(keyword in title for keyword in business_keywords):
                categories['business'].append(story)
            else:
                categories['others'].append(story)
        
        return categories
    
    def _write_story_entry(self, file, idx, story):
        """
        å°†æ•…äº‹æ¡ç›®å†™å…¥æ–‡ä»¶
        
        Args:
            file: æ–‡ä»¶å¯¹è±¡
            idx: ç´¢å¼•
            story: æ•…äº‹å¯¹è±¡
        """
        if not story:
            return
            
        title = story.get('title', 'æ— æ ‡é¢˜')
        story_type = story.get('type', 'unknown')
        
        # æ ¹æ®ä¸åŒç±»å‹æ·»åŠ ä¸åŒçš„å›¾æ ‡
        icon = "ğŸ“°"
        if "show hn" in title.lower():
            icon = "ğŸ”"
        elif "ask hn" in title.lower():
            icon = "â“"
        elif story_type == "job":
            icon = "ğŸ’¼"
        
        # å¤„ç†ä¸åŒç±»å‹çš„æ•…äº‹
        if 'url' in story:
            # æ™®é€šçš„å¸¦å¤–éƒ¨é“¾æ¥çš„æ•…äº‹
            url = story['url']
            file.write(f"{icon} **[{title}]({url})**\n")
        else:
            # Ask HN, Show HN æˆ–å…¶ä»–æ²¡æœ‰å¤–éƒ¨URLçš„æ•…äº‹ç±»å‹
            hn_item_url = f"https://news.ycombinator.com/item?id={story.get('id', '')}"
            file.write(f"{icon} **[{title}]({hn_item_url})**\n")
        
        # æ·»åŠ AIæ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰
        if 'ai_summary' in story and story['ai_summary']:
            file.write(f"  ğŸ“ {story['ai_summary']}\n")
        
        # æ·»åŠ é¢å¤–ä¿¡æ¯ï¼ˆåˆ†æ•°ã€å‘å¸ƒè€…ã€æ—¶é—´ç­‰ï¼‰
        extra_info = []
        if 'score' in story:
            extra_info.append(f"ğŸ‘ **{story['score']}** åˆ†")
        if 'by' in story:
            extra_info.append(f"ğŸ‘¤ ä½œè€…: {story['by']}")
        if 'descendants' in story and story['descendants'] > 0:
            extra_info.append(f"ğŸ’¬ {story['descendants']} æ¡è¯„è®º")
            
        if extra_info:
            file.write(f"  {' | '.join(extra_info)}\n\n")
        else:
            file.write("\n")
    
    def _analyze_topics(self, stories, date=None, hour=None):
        """
        åˆ†ææ•…äº‹ä¸»é¢˜å¹¶ç”ŸæˆæŠ¥å‘Š
        
        Args:
            stories: HNæ•…äº‹åˆ—è¡¨
            date: æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)
            hour: å°æ—¶å­—ç¬¦ä¸² (HH)
            
        Returns:
            ç”Ÿæˆçš„ä¸»é¢˜æŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœå‘ç”Ÿé”™è¯¯åˆ™è¿”å›None
        """
        try:
            LOG.info("å‡†å¤‡å¯¼å…¥è¯é¢˜åˆ†æå™¨...")
            # ç¡®ä¿NLTKæ•°æ®å·²ä¸‹è½½
            import nltk
            nltk_dirs = ["tokenizers/punkt", "corpora/stopwords", "corpora/wordnet"]
            for nltk_dir in nltk_dirs:
                try:
                    nltk.data.find(nltk_dir)
                except LookupError:
                    LOG.info(f"ä¸‹è½½NLTKæ•°æ®: {nltk_dir}...")
                    nltk.download(nltk_dir.split('/')[-1])
            
            # å»¶è¿Ÿå¯¼å…¥ï¼Œç¡®ä¿å³ä½¿æ²¡æœ‰å®‰è£…åˆ†æå™¨ä¾èµ–ä¹Ÿèƒ½è¿è¡ŒåŸºæœ¬åŠŸèƒ½
            from src.analyzers.topic_analyzer import HackerNewsTopicAnalyzer
            
            LOG.info("å¼€å§‹åˆ†æHacker Newsä¸»é¢˜...")
            analyzer = HackerNewsTopicAnalyzer()
            
            # åˆ†æè¯é¢˜
            result = analyzer.analyze_topics(stories, date, hour)
            
            # ç”ŸæˆæŠ¥å‘Š
            report = analyzer.generate_report(result)
            
            # ä¿å­˜æŠ¥å‘Š
            if date is None:
                date = datetime.now().strftime('%Y-%m-%d')
            if hour is None:
                hour = datetime.now().strftime('%H')
                
            dir_path = os.path.join('hacker_news', date)
            os.makedirs(dir_path, exist_ok=True)
            
            topics_file_path = os.path.join(dir_path, f'{hour}_topics.md')
            with open(topics_file_path, 'w', encoding='utf-8') as f:
                f.write(report)
                
            LOG.info(f"Hacker Newsä¸»é¢˜åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {topics_file_path}")
            return topics_file_path
            
        except ImportError as e:
            LOG.warning(f"æœªèƒ½å¯¼å…¥ä¸»é¢˜åˆ†æå™¨ï¼Œè·³è¿‡ä¸»é¢˜åˆ†æ (è¯·å®‰è£…å¿…è¦çš„ä¾èµ–åº“): {e}")
            return None
        except Exception as e:
            LOG.error(f"åˆ†æHacker Newsä¸»é¢˜æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            LOG.error(traceback.format_exc())
            return None
    
    def _add_ai_summaries(self, stories: List[Dict[str, Any]]) -> bool:
        """
        ä½¿ç”¨LLMä¸ºçƒ­é—¨æ•…äº‹æ·»åŠ AIç”Ÿæˆçš„æ‘˜è¦
        
        Args:
            stories: éœ€è¦æ·»åŠ æ‘˜è¦çš„æ•…äº‹åˆ—è¡¨
            
        Returns:
            æ˜¯å¦æˆåŠŸæ·»åŠ æ‘˜è¦
        """
        try:
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥å¯¼å…¥LLM
            try:
                from src.llm import LLM
                from src.config import Settings
            except ImportError:
                LOG.warning("æœªèƒ½å¯¼å…¥LLMæˆ–Settingsæ¨¡å—ï¼Œè·³è¿‡AIæ€»ç»“")
                return False
                
            # åˆå§‹åŒ–LLM
            try:
                settings = Settings(config_file="config.json")
                llm = LLM(settings=settings)
            except Exception as e:
                LOG.warning(f"åˆå§‹åŒ–LLMå¤±è´¥ï¼Œè·³è¿‡AIæ€»ç»“: {e}")
                return False
            
            LOG.info(f"å¼€å§‹ä¸º{len(stories)}ä¸ªçƒ­é—¨æ•…äº‹ç”ŸæˆAIæ‘˜è¦...")
            
            # ä¸ºæ¯ä¸ªæ•…äº‹ç”Ÿæˆæ‘˜è¦
            for story in stories:
                # å¦‚æœå·²ç»æœ‰æ‘˜è¦ï¼Œè·³è¿‡
                if 'ai_summary' in story:
                    continue
                    
                title = story.get('title', '')
                url = story.get('url', '')
                
                # æ„å»ºæç¤º
                prompt = f"""
                è¯·ä¸ºä»¥ä¸‹Hacker Newsæ–‡ç« ç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„ä¸­æ–‡æ‘˜è¦ï¼ˆä¸è¶…è¿‡30ä¸ªå­—ï¼‰ã€‚
                æ ‡é¢˜: {title}
                é“¾æ¥: {url if url else 'æ— å¤–éƒ¨é“¾æ¥'}
                
                ä½ çš„å›ç­”åº”è¯¥æ˜¯çº¯æ–‡æœ¬æ ¼å¼ï¼Œç›´æ¥æè¿°è¿™ç¯‡æ–‡ç« å¤§æ¦‚è®²äº†ä»€ä¹ˆï¼Œä¸è¦åŒ…å«"è¿™ç¯‡æ–‡ç« è®²è¿°äº†"ä¹‹ç±»çš„å¼•å¯¼è¯­ã€‚
                """
                
                try:
                    # ä½¿ç”¨LLMç”Ÿæˆæ‘˜è¦ - ä¿®å¤æ–¹æ³•åç§°
                    summary_chunks = list(llm.generate_report(
                        system_prompt="ä½ æ˜¯ä¸€ä¸ªç®€æ´çš„æ–‡ç« æ‘˜è¦åŠ©æ‰‹ï¼Œä½ çš„ä»»åŠ¡æ˜¯ç”Ÿæˆç®€çŸ­çš„ä¸­æ–‡æ‘˜è¦ã€‚",
                        user_content=prompt
                    ))
                    clean_summary = "".join(summary_chunks).strip()
                    
                    # ç¡®ä¿æ‘˜è¦ä¸è¶…è¿‡40ä¸ªå­—
                    if len(clean_summary) > 40:
                        clean_summary = clean_summary[:37] + "..."
                    
                    # ä¿å­˜æ‘˜è¦
                    story['ai_summary'] = clean_summary
                    LOG.debug(f"å·²ä¸ºæ–‡ç« '{title}'ç”Ÿæˆæ‘˜è¦: {clean_summary}")
                    
                    # é¿å…è¿‡äºé¢‘ç¹çš„APIè°ƒç”¨
                    time.sleep(0.5)
                    
                except Exception as e:
                    LOG.warning(f"ä¸ºæ–‡ç« '{title}'ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}")
                    continue
            
            return True
            
        except Exception as e:
            LOG.error(f"æ·»åŠ AIæ‘˜è¦è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            LOG.error(traceback.format_exc())
            return False
    
    async def async_export_top_stories(self, date=None, hour=None, enable_ai_summary=True):
        """
        å¼‚æ­¥å¯¼å‡ºå½“å‰ Hacker News çƒ­é—¨æ•…äº‹åˆ—è¡¨åˆ° Markdown æ–‡ä»¶
        
        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD æ ¼å¼)ï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨å½“å‰æ—¥æœŸ
            hour: å°æ—¶å­—ç¬¦ä¸² (HH æ ¼å¼)ï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨å½“å‰å°æ—¶
            enable_ai_summary: æ˜¯å¦å¯ç”¨AIæ‘˜è¦åŠŸèƒ½ï¼Œé»˜è®¤ä¸ºTrue
            
        Returns:
            ç”Ÿæˆçš„ Markdown æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœå‘ç”Ÿé”™è¯¯åˆ™è¿”å› None
        """
        LOG.debug("å‡†å¤‡å¼‚æ­¥å¯¼å‡ºHacker Newsçš„çƒ­é—¨æ–°é—»ã€‚")
        
        try:
            # è·å–æ–°é—»æ•°æ®
            stories_details = await self.async_get_top_stories_details(limit=30)
            
            if not stories_details:
                LOG.warning("å¼‚æ­¥è·å–æœªæ‰¾åˆ°ä»»ä½•Hacker Newsçš„æ–°é—»ã€‚")
                return None
            
            # å¦‚æœæœªæä¾› date å’Œ hour å‚æ•°ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸå’Œæ—¶é—´
            if date is None:
                date = datetime.now().strftime('%Y-%m-%d')
            if hour is None:
                hour = datetime.now().strftime('%H')
            
            # æ„å»ºå­˜å‚¨è·¯å¾„
            dir_path = os.path.join('hacker_news', date)
            os.makedirs(dir_path, exist_ok=True)
            
            # å¯¹æ•…äº‹è¿›è¡Œåˆ†ç±»
            categorized_stories = self._categorize_stories(stories_details)
            
            # å°è¯•ä¸ºçƒ­é—¨æ•…äº‹æ·»åŠ AIæ‘˜è¦
            if enable_ai_summary:
                LOG.info("AIæ‘˜è¦åŠŸèƒ½å·²å¯ç”¨ï¼Œæ­£åœ¨ä¸ºçƒ­é—¨æ–‡ç« ç”Ÿæˆæ‘˜è¦...")
                top_stories = sorted(stories_details, key=lambda x: x.get('score', 0), reverse=True)[:10]
                self._add_ai_summaries(top_stories)
            else:
                LOG.info("AIæ‘˜è¦åŠŸèƒ½å·²ç¦ç”¨ï¼Œè·³è¿‡æ‘˜è¦ç”Ÿæˆ")
            
            # ä¿å­˜åŸå§‹åˆ—è¡¨
            stories_file_path = os.path.join(dir_path, f'{hour}.md')
            with open(stories_file_path, 'w', encoding='utf-8') as file:
                file.write(f"# Hacker News çƒ­é—¨æ–°é—» ({date} {hour}:00)\n\n")
                file.write("æœ¬æŠ¥å‘Šæ”¶é›†äº†æœ€è¿‘ä¸€å°æ—¶å†…Hacker Newså¹³å°ä¸Šçš„çƒ­é—¨è®¨è®ºå†…å®¹ã€‚\"åˆ†æ•°\"ä»£è¡¨ç”¨æˆ·çš„æŠ•ç¥¨æ•°é‡ï¼Œåæ˜ äº†æ–‡ç« çš„çƒ­åº¦ä¸å—æ¬¢è¿ç¨‹åº¦ã€‚\n\n")
                
                # é¦–å…ˆæ˜¾ç¤ºæœ€çƒ­é—¨çš„æ•…äº‹
                file.write("## ğŸ”¥ æœ€çƒ­é—¨è®¨è®º\n\n")
                # æŒ‰åˆ†æ•°æ’åºï¼Œå–å‰3ä¸ª
                top_stories = sorted(stories_details, key=lambda x: x.get('score', 0), reverse=True)[:3]
                for idx, story in enumerate(top_stories, start=1):
                    self._write_story_entry(file, idx, story)
                
                # æ·»åŠ å±•ç¤ºç±»å†…å®¹
                if categorized_stories['show_hn']:
                    file.write("\n## ğŸ‘€ å±•ç¤ºä¸åˆ†äº«\n\n")
                    file.write("ç”¨æˆ·åˆ†äº«çš„é¡¹ç›®ã€äº§å“å’Œåˆ›é€ ã€‚\n\n")
                    for idx, story in enumerate(categorized_stories['show_hn'], start=1):
                        self._write_story_entry(file, idx, story)
                
                # æ·»åŠ é—®ç­”ç±»å†…å®¹
                if categorized_stories['ask_hn']:
                    file.write("\n## â“ é—®ç­”ä¸è®¨è®º\n\n")
                    file.write("ç¤¾åŒºæˆå‘˜æå‡ºçš„é—®é¢˜å’Œè®¨è®ºã€‚\n\n")
                    for idx, story in enumerate(categorized_stories['ask_hn'], start=1):
                        self._write_story_entry(file, idx, story)
                
                # æ·»åŠ æŠ€æœ¯ç±»å†…å®¹
                if categorized_stories['tech']:
                    file.write("\n## ğŸ’» æŠ€æœ¯ä¸å·¥ç¨‹\n\n")
                    file.write("ç¼–ç¨‹ã€å¼€å‘å’ŒæŠ€æœ¯ç›¸å…³çš„çƒ­é—¨è¯é¢˜ã€‚\n\n")
                    for idx, story in enumerate(categorized_stories['tech'], start=1):
                        self._write_story_entry(file, idx, story)
                
                # æ·»åŠ ç§‘å­¦ç±»å†…å®¹
                if categorized_stories['science']:
                    file.write("\n## ğŸ”¬ ç§‘å­¦ç ”ç©¶\n\n")
                    file.write("ç§‘å­¦å‘ç°ã€ç ”ç©¶å’Œç›¸å…³è®¨è®ºã€‚\n\n")
                    for idx, story in enumerate(categorized_stories['science'], start=1):
                        self._write_story_entry(file, idx, story)
                
                # æ·»åŠ å•†ä¸šç±»å†…å®¹
                if categorized_stories['business']:
                    file.write("\n## ğŸ’¼ å•†ä¸šä¸åˆ›ä¸š\n\n")
                    file.write("å•†ä¸šæ–°é—»ã€åˆ›ä¸šå’Œè¡Œä¸šåŠ¨æ€ã€‚\n\n")
                    for idx, story in enumerate(categorized_stories['business'], start=1):
                        self._write_story_entry(file, idx, story)
                
                # æ·»åŠ å…¶ä»–ç±»å†…å®¹
                if categorized_stories['others']:
                    file.write("\n## ğŸ“š å…¶ä»–çƒ­é—¨å†…å®¹\n\n")
                    for idx, story in enumerate(categorized_stories['others'], start=1):
                        self._write_story_entry(file, idx, story)
            
            # æ‰§è¡Œä¸»é¢˜åˆ†æ (ä½¿ç”¨åŒæ­¥ç‰ˆæœ¬ï¼Œå› ä¸ºåˆ†æå™¨å°šæœªå¼‚æ­¥åŒ–)
            topics_file_path = self._analyze_topics(stories_details, date, hour)
            
            LOG.info(f"Hacker Newsçƒ­é—¨æ–°é—»æ–‡ä»¶å¼‚æ­¥ç”Ÿæˆï¼š{stories_file_path}")
            return stories_file_path
            
        except Exception as e:
            LOG.error(f"å¼‚æ­¥å¯¼å‡ºHacker Newsçƒ­é—¨æ–°é—»æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            LOG.error(traceback.format_exc())
            return None
    
    def clear_cache(self):
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
        if self.use_cache:
            count = self.cache.clear_all()
            LOG.info(f"å·²æ¸…é™¤ {count} ä¸ªHacker Newsç¼“å­˜æ–‡ä»¶")
            return count
        return 0


if __name__ == "__main__":
    client = HackerNewsClient()
    client.export_top_stories()  # é»˜è®¤æƒ…å†µä¸‹ä½¿ç”¨å½“å‰æ—¥æœŸå’Œæ—¶é—´
