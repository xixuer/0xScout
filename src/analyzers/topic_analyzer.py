"""
Hacker News è¯é¢˜åˆ†ææ¨¡å—
ç”¨äºåˆ†æå’Œèšç±» Hacker News æ–‡ç« ï¼Œè¯†åˆ«çƒ­é—¨è¯é¢˜å’Œè¶‹åŠ¿
"""

import os
import re
import json
import time
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from collections import defaultdict, Counter
from typing import List, Dict, Any, Optional, Tuple, Set

# NLPç›¸å…³åº“
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.util import ngrams

# æœºå™¨å­¦ä¹ ç›¸å…³åº“
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.cluster import DBSCAN, KMeans
from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation
from sklearn.metrics.pairwise import cosine_similarity

# å¯¼å…¥æ—¥å¿—
try:
    from src.logger import LOG
except ImportError:
    import logging
    LOG = logging.getLogger(__name__)

# ç¡®ä¿NLTKæ•°æ®å·²ä¸‹è½½
def ensure_nltk_data():
    """ç¡®ä¿NLTKå¿…è¦çš„æ•°æ®åŒ…å·²ä¸‹è½½"""
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        LOG.info("ä¸‹è½½NLTK punktæ•°æ®åŒ…...")
        nltk.download('punkt', quiet=True)
    
    try:
        nltk.data.find('corpora/stopwords')
    except LookupError:
        LOG.info("ä¸‹è½½NLTK stopwordsæ•°æ®åŒ…...")
        nltk.download('stopwords', quiet=True)
    
    try:
        nltk.data.find('corpora/wordnet')
    except LookupError:
        LOG.info("ä¸‹è½½NLTK wordnetæ•°æ®åŒ…...")
        nltk.download('wordnet', quiet=True)
        
    # æ·»åŠ punkt_tabä¸‹è½½
    try:
        nltk.data.find('tokenizers/punkt_tab')
    except LookupError:
        LOG.info("ä¸‹è½½NLTK punkt_tabæ•°æ®åŒ…...")
        try:
            nltk.download('punkt', quiet=True)  # punktåŒ…å«punkt_tabæ‰€éœ€çš„æ•°æ®
        except Exception as e:
            LOG.warning(f"æ— æ³•ä¸‹è½½punkt_tabæ•°æ®åŒ…: {e}")
            # ä¿®æ”¹ä½¿ç”¨æ›¿ä»£æ–¹æ³•
            import nltk.tokenize as nt
            # ç¡®ä¿punkt_tokenizeå¯ç”¨ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆ
            nltk.tokenize.sent_tokenize = lambda text, language='english': text.split('.')
            LOG.info("å·²ä½¿ç”¨æ›¿ä»£åˆ†å¥æ–¹æ³•æ›¿æ¢punkt_tabåŠŸèƒ½")


class HackerNewsTopicAnalyzer:
    """Hacker News è¯é¢˜åˆ†æå™¨"""
    
    def __init__(self, cache_dir='cache/topic_analysis'):
        """
        åˆå§‹åŒ–è¯é¢˜åˆ†æå™¨
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•ï¼Œç”¨äºå­˜å‚¨å†å²è¯é¢˜æ•°æ®
        """
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        
        # åˆå§‹åŒ–NLPç»„ä»¶
        ensure_nltk_data()
        self.stop_words = set(stopwords.words('english'))
        self.tech_stop_words = {'use', 'using', 'used', 'new', 'data', 'create', 'like', 
                               'simple', 'build', 'building', 'built', 'tech', 'technology'}
        self.stop_words.update(self.tech_stop_words)
        
        # æ·»åŠ ä¸€äº›æŠ€æœ¯æœ¯è¯­å’Œå…¬å¸åç§°åˆ°è¯åº“
        self.tech_entities = {
            'javascript', 'python', 'java', 'cpp', 'c++', 'rust', 'golang', 'go', 
            'typescript', 'react', 'angular', 'vue', 'node', 'nodejs', 'django', 'flask',
            'ai', 'ml', 'deeplearning', 'machinelearning', 'llm', 'gpt', 'openai', 'github',
            'aws', 'azure', 'google', 'microsoft', 'apple', 'facebook', 'meta', 'amazon',
            'blockchain', 'crypto', 'bitcoin', 'ethereum', 'nft', 'web3',
            'frontend', 'backend', 'fullstack', 'devops', 'database', 'api'
        }
        
        self.stemmer = PorterStemmer()
        self.lemmatizer = WordNetLemmatizer()
        
        # å‘é‡åŒ–å™¨
        self.vectorizer = TfidfVectorizer(
            max_features=1000,  # é™åˆ¶ç‰¹å¾æ•°é‡ï¼Œé€‚åº”å†…å­˜é™åˆ¶
            min_df=2,           # è‡³å°‘åœ¨2ç¯‡æ–‡ç« ä¸­å‡ºç°
            max_df=0.8,         # åœ¨ä¸è¶…è¿‡80%çš„æ–‡ç« ä¸­å‡ºç°
            ngram_range=(1, 2)  # åŒæ—¶è€ƒè™‘å•è¯å’ŒåŒè¯ç»„åˆ
        )
        
        # å†å²è¯é¢˜æ•°æ®
        self.historical_topics = {}
    
    def preprocess_text(self, text):
        """
        æ–‡æœ¬é¢„å¤„ç†
        
        Args:
            text: åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²
            
        Returns:
            é¢„å¤„ç†åçš„å­—ç¬¦ä¸²
        """
        if not text:
            return ""
            
        # è½¬ä¸ºå°å†™
        text = text.lower()
        
        # ç§»é™¤URL
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', text)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ•°å­—
        text = re.sub(r'[^a-zA-Z\s]', ' ', text)
        
        # æ ‡è®°åŒ– - é¿å…ä½¿ç”¨word_tokenizeï¼ˆä¾èµ–äºpunkt_tabï¼‰
        try:
            # å°è¯•ä½¿ç”¨word_tokenize
            tokens = word_tokenize(text)
        except LookupError:
            # å›é€€åˆ°ç®€å•çš„ç©ºæ ¼åˆ†å‰²
            LOG.warning("æ— æ³•ä½¿ç”¨NLTK word_tokenizeï¼Œå›é€€åˆ°åŸºæœ¬åˆ†è¯")
            tokens = text.split()
        
        # è¿‡æ»¤åœç”¨è¯
        tokens = [t for t in tokens if t not in self.stop_words and len(t) > 2]
        
        # è¯å½¢è¿˜åŸ (æ›´è½»é‡çº§ï¼Œæ¯”è¯å¹²æå–æ›´ç¬¦åˆè¯­ä¹‰)
        tokens = [self.lemmatizer.lemmatize(t) for t in tokens]
        
        # è¯†åˆ«æŠ€æœ¯å®ä½“ (ä¿ç•™å®Œæ•´å½¢å¼)
        for i, token in enumerate(tokens):
            if token in self.tech_entities:
                tokens[i] = f"TECH_{token.upper()}"
        
        return ' '.join(tokens)
    
    def preprocess_stories(self, stories):
        """
        é¢„å¤„ç†HackerNewsæ•…äº‹
        
        Args:
            stories: HNæ•…äº‹åˆ—è¡¨
            
        Returns:
            é¢„å¤„ç†åçš„æ–‡æœ¬å’Œå…ƒæ•°æ®
        """
        processed_texts = []
        metadata = []
        
        for story in stories:
            # æå–ç›¸å…³å­—æ®µ
            story_id = story.get('id')
            title = story.get('title', '')
            url = story.get('url', '')
            text = story.get('text', '')  # Ask HNç­‰å¸–å­çš„æ–‡æœ¬å†…å®¹
            
            # è§£æURLçš„åŸŸåä½œä¸ºé¢å¤–ä¿¡æ¯
            domain = ''
            if url:
                try:
                    from urllib.parse import urlparse
                    domain = urlparse(url).netloc
                except:
                    pass
            
            # ç»„åˆæ ‡é¢˜å’Œæ–‡æœ¬
            combined_text = f"{title} {text} {domain}"
            
            # é¢„å¤„ç†
            processed_text = self.preprocess_text(combined_text)
            
            processed_texts.append(processed_text)
            metadata.append({
                'id': story_id,
                'title': title,
                'url': url,
                'score': story.get('score', 0),
                'by': story.get('by', ''),
                'time': story.get('time', 0),
                'descendants': story.get('descendants', 0)  # è¯„è®ºæ•°
            })
        
        return processed_texts, metadata
    
    def cluster_topics(self, processed_texts, metadata, n_clusters=None):
        """
        èšç±»è¯é¢˜
        
        Args:
            processed_texts: é¢„å¤„ç†åçš„æ–‡æœ¬åˆ—è¡¨
            metadata: æ•…äº‹å…ƒæ•°æ®
            n_clusters: èšç±»æ•°é‡ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç¡®å®š
            
        Returns:
            èšç±»ç»“æœ
        """
        if not processed_texts:
            return [], []
        
        # å‘é‡åŒ–
        try:
            X = self.vectorizer.fit_transform(processed_texts)
        except ValueError:
            # å¦‚æœæ–‡æœ¬éƒ½æ˜¯ç©ºçš„ï¼Œè¿”å›ç©ºç»“æœ
            LOG.warning("æ— æ³•å‘é‡åŒ–æ–‡æœ¬ï¼Œå¯èƒ½å…¨æ˜¯åœç”¨è¯")
            return [0] * len(processed_texts), []
        
        # å¦‚æœæ•°æ®å¤ªå°‘ï¼Œæ‰€æœ‰æ•…äº‹å½’ä¸ºä¸€ä¸ªè¯é¢˜
        if len(processed_texts) < 3:
            LOG.info("æ•…äº‹æ•°é‡å¤ªå°‘ï¼Œå…¨éƒ¨å½’ä¸ºä¸€ä¸ªè¯é¢˜")
            return [0] * len(processed_texts), [self.extract_topic_keywords(X, [0] * len(processed_texts), 0)]
        
        # é™ç»´ä»¥åŠ é€Ÿèšç±» (é€‚åˆèµ„æºå—é™ç¯å¢ƒ)
        if X.shape[1] > 100:
            svd = TruncatedSVD(n_components=min(50, X.shape[0] - 1))
            X_reduced = svd.fit_transform(X)
        else:
            X_reduced = X.toarray()
        
        # åˆ¤æ–­ä½¿ç”¨çš„èšç±»ç®—æ³•
        if n_clusters is None:
            # ä½¿ç”¨DBSCANè‡ªåŠ¨ç¡®å®šèšç±»æ•°
            clustering = DBSCAN(eps=0.5, min_samples=2)
            labels = clustering.fit_predict(X_reduced)
            
            # å¦‚æœå¤§éƒ¨åˆ†ä¸ºå™ªå£°ç‚¹ (-1)ï¼Œå°è¯•è°ƒæ•´å‚æ•°
            if (labels == -1).sum() > len(labels) * 0.5:
                LOG.debug("DBSCANèšç±»äº§ç”Ÿè¿‡å¤šå™ªå£°ç‚¹ï¼Œå°è¯•K-Means")
                # ä½¿ç”¨K-Meansï¼Œèšç±»æ•°é‡ä¼°è®¡ä¸ºæ–‡ç« æ•°çš„1/3ï¼Œæœ€å°‘3ä¸ªï¼Œæœ€å¤š7ä¸ª
                k = max(3, min(7, len(processed_texts) // 3))
                clustering = KMeans(n_clusters=k, random_state=42)
                labels = clustering.fit_predict(X_reduced)
        else:
            # æŒ‡å®šèšç±»æ•°é‡
            clustering = KMeans(n_clusters=n_clusters, random_state=42)
            labels = clustering.fit_predict(X_reduced)
        
        # æå–æ¯ä¸ªèšç±»çš„å…³é”®è¯
        topics = []
        unique_labels = sorted(set(labels))
        for label in unique_labels:
            if label == -1:  # è·³è¿‡å™ªå£°ç‚¹
                continue
            topics.append(self.extract_topic_keywords(X, labels, label))
        
        return labels, topics
    
    def extract_topic_keywords(self, X, labels, label_id, top_n=5):
        """
        æå–èšç±»çš„å…³é”®è¯
        
        Args:
            X: TF-IDFçŸ©é˜µ
            labels: èšç±»æ ‡ç­¾
            label_id: å½“å‰èšç±»çš„ID
            top_n: è¿”å›çš„å…³é”®è¯æ•°é‡
            
        Returns:
            è¯é¢˜å…³é”®è¯å’Œæƒé‡
        """
        if hasattr(X, 'toarray'):
            X = X.toarray()
        
        # è·å–è¯¥èšç±»çš„æ‰€æœ‰æ–‡æ¡£
        cluster_docs = np.where(np.array(labels) == label_id)[0]
        
        if len(cluster_docs) == 0:
            return {'keywords': []}
        
        # è·å–è¯¥èšç±»çš„TF-IDFå‡å€¼
        centroid = X[cluster_docs].mean(axis=0)
        
        # è·å–ç‰¹å¾åç§°
        if hasattr(self.vectorizer, 'get_feature_names_out'):
            feature_names = self.vectorizer.get_feature_names_out()
        else:
            feature_names = self.vectorizer.get_feature_names()
        
        # æ’åºå¹¶è·å–topå…³é”®è¯
        indices = centroid.argsort()[-top_n:][::-1]
        keywords = [(feature_names[i], float(centroid[i])) for i in indices]
        
        return {
            'id': label_id,
            'keywords': keywords,
            'size': len(cluster_docs)
        }
    
    def analyze_topics(self, stories, date=None, hour=None):
        """
        åˆ†ææ•…äº‹è¯é¢˜
        
        Args:
            stories: HNæ•…äº‹åˆ—è¡¨
            date: æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)
            hour: å°æ—¶å­—ç¬¦ä¸² (HH)
            
        Returns:
            è¯é¢˜åˆ†æç»“æœ
        """
        if not stories:
            return {
                'topics': [],
                'stories_by_topic': {},
                'trends': {
                    'emerging': [],
                    'continuing': [],
                    'fading': []
                }
            }
        
        # ç¡®å®šæ—¶é—´
        if date is None:
            date = datetime.now().strftime('%Y-%m-%d')
        if hour is None:
            hour = datetime.now().strftime('%H')
            
        LOG.info(f"åˆ†æ {date} {hour}:00 çš„Hacker Newsè¯é¢˜")
        
        # é¢„å¤„ç†æ–‡æœ¬
        processed_texts, metadata = self.preprocess_stories(stories)
        
        # èšç±»
        labels, topics = self.cluster_topics(processed_texts, metadata)
        
        # å°†æ•…äº‹æŒ‰è¯é¢˜åˆ†ç»„
        stories_by_topic = defaultdict(list)
        for i, label in enumerate(labels):
            if label != -1:  # å¿½ç•¥å™ªå£°ç‚¹
                topic_idx = list(set(labels)).index(label) if label in set(labels) else -1
                if topic_idx >= 0 and topic_idx < len(topics):
                    stories_by_topic[topic_idx].append(metadata[i])
        
        # åŠ è½½å†å²è¯é¢˜ç”¨äºè¶‹åŠ¿åˆ†æ
        prev_hour_key = self._get_previous_hour_key(date, hour)
        historical_topics = self._load_historical_topics(prev_hour_key)
        
        # è¶‹åŠ¿åˆ†æ
        trends = self._analyze_trends(topics, historical_topics)
        
        # ä¿å­˜å½“å‰è¯é¢˜
        current_key = f"{date}_{hour}"
        self._save_topics(current_key, topics)
        
        return {
            'topics': topics,
            'stories_by_topic': dict(stories_by_topic),
            'trends': trends,
            'date': date,
            'hour': hour
        }
    
    def _get_previous_hour_key(self, date, hour):
        """è·å–å‰ä¸€å°æ—¶çš„é”®"""
        dt = datetime.strptime(f"{date} {hour}:00:00", "%Y-%m-%d %H:%M:%S")
        prev_dt = dt - timedelta(hours=1)
        return f"{prev_dt.strftime('%Y-%m-%d')}_{prev_dt.strftime('%H')}"
    
    def _load_historical_topics(self, key):
        """åŠ è½½å†å²è¯é¢˜æ•°æ®"""
        file_path = os.path.join(self.cache_dir, f"{key}.json")
        if os.path.exists(file_path):
            try:
                with open(file_path, 'r') as f:
                    return json.load(f)
            except:
                LOG.warning(f"æ— æ³•åŠ è½½å†å²è¯é¢˜æ•°æ®ï¼š{file_path}")
        return []
    
    def _save_topics(self, key, topics):
        """ä¿å­˜å½“å‰è¯é¢˜æ•°æ®"""
        file_path = os.path.join(self.cache_dir, f"{key}.json") 
        try:
            with open(file_path, 'w') as f:
                json.dump(topics, f)
        except:
            LOG.warning(f"æ— æ³•ä¿å­˜è¯é¢˜æ•°æ®ï¼š{file_path}")
    
    def _analyze_trends(self, current_topics, historical_topics):
        """åˆ†æè¯é¢˜è¶‹åŠ¿"""
        if not historical_topics or not current_topics:
            # æ²¡æœ‰å†å²æ•°æ®æˆ–å½“å‰è¯é¢˜ä¸ºç©ºï¼Œæ‰€æœ‰è¯é¢˜éƒ½æ˜¯æ–°å…´çš„
            return {
                'emerging': list(range(len(current_topics))),
                'continuing': [],
                'fading': []
            }
        
        # è®¡ç®—è¯é¢˜ç›¸ä¼¼åº¦
        similarity_matrix = self._compute_topic_similarity(current_topics, historical_topics)
        
        # è¯†åˆ«è¶‹åŠ¿
        emerging = []
        continuing = []
        fading_hist_indices = list(range(len(historical_topics)))
        
        threshold = 0.3  # è¯é¢˜ç›¸ä¼¼åº¦é˜ˆå€¼
        
        # å¯¹æ¯ä¸ªå½“å‰è¯é¢˜
        for i, topic in enumerate(current_topics):
            max_sim = 0
            max_idx = -1
            
            # æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„å†å²è¯é¢˜
            for j, hist_topic in enumerate(historical_topics):
                if similarity_matrix[i][j] > max_sim:
                    max_sim = similarity_matrix[i][j]
                    max_idx = j
            
            if max_sim >= threshold:
                # è¯é¢˜æŒç»­
                continuing.append({
                    'current_idx': i,
                    'historical_idx': max_idx,
                    'similarity': max_sim
                })
                if max_idx in fading_hist_indices:
                    fading_hist_indices.remove(max_idx)
            else:
                # æ–°è¯é¢˜
                emerging.append(i)
        
        return {
            'emerging': emerging,
            'continuing': continuing,
            'fading': fading_hist_indices
        }
    
    def _compute_topic_similarity(self, topics1, topics2):
        """è®¡ç®—ä¸¤ç»„è¯é¢˜é—´çš„ç›¸ä¼¼åº¦"""
        similarity_matrix = []
        
        for topic1 in topics1:
            row = []
            keywords1 = dict(topic1.get('keywords', []))
            
            for topic2 in topics2:
                keywords2 = dict(topic2.get('keywords', []))
                
                # è®¡ç®—å…±åŒå…³é”®è¯çš„ç›¸ä¼¼åº¦
                common_keywords = set(keywords1.keys()) & set(keywords2.keys())
                if not common_keywords:
                    row.append(0)
                    continue
                
                # è®¡ç®—å…±åŒå…³é”®è¯çš„æƒé‡ç›¸ä¼¼åº¦
                similarity = 0
                for keyword in common_keywords:
                    similarity += min(keywords1.get(keyword, 0), keywords2.get(keyword, 0))
                
                # å½’ä¸€åŒ–
                norm1 = sum(keywords1.values())
                norm2 = sum(keywords2.values())
                if norm1 and norm2:
                    similarity /= (norm1 * norm2) ** 0.5
                
                row.append(similarity)
            
            similarity_matrix.append(row)
        
        return similarity_matrix
    
    def generate_report(self, analysis_result):
        """
        æ ¹æ®åˆ†æç»“æœç”ŸæˆæŠ¥å‘Š
        
        Args:
            analysis_result: è¯é¢˜åˆ†æç»“æœ
            
        Returns:
            Markdownæ ¼å¼çš„æŠ¥å‘Š
        """
        if not analysis_result or not analysis_result.get('topics'):
            return "æœªå‘ç°ä»»ä½•è¯é¢˜ã€‚"
        
        topics = analysis_result['topics']
        stories_by_topic = analysis_result.get('stories_by_topic', {})
        trends = analysis_result.get('trends', {})
        date = analysis_result.get('date', datetime.now().strftime('%Y-%m-%d'))
        hour = analysis_result.get('hour', datetime.now().strftime('%H'))
        
        # æŠ¥å‘Šå¤´éƒ¨
        report = [f"# Hacker News è¯é¢˜åˆ†æ ({date} {hour}:00)\n"]
        
        # æ€»ç»“éƒ¨åˆ†
        report.append(f"## æ€»ä½“æ¦‚å†µ\n")
        report.append(f"æœ¬å°æ—¶å…±å‘ç° **{len(topics)}** ä¸ªä¸»è¦è¯é¢˜ã€‚\n")
        
        if trends['emerging']:
            report.append(f"- æ–°å…´è¯é¢˜: **{len(trends['emerging'])}** ä¸ª\n")
        if trends['continuing']:
            report.append(f"- æŒç»­è¯é¢˜: **{len(trends['continuing'])}** ä¸ª\n")
        
        # æ¯ä¸ªè¯é¢˜è¯¦æƒ…
        report.append(f"\n## è¯é¢˜è¯¦æƒ…\n")
        
        # é¦–å…ˆå¤„ç†æ–°å…´è¯é¢˜
        if trends['emerging']:
            report.append(f"### æ–°å…´è¯é¢˜\n")
            for topic_idx in trends['emerging']:
                if topic_idx < len(topics):
                    topic = topics[topic_idx]
                    report.append(self._format_topic_section(topic, stories_by_topic.get(topic_idx, []), is_new=True))
        
        # ç„¶åå¤„ç†æŒç»­è¯é¢˜
        if trends['continuing']:
            report.append(f"### æŒç»­çƒ­é—¨è¯é¢˜\n")
            for cont in trends['continuing']:
                topic_idx = cont['current_idx']
                if topic_idx < len(topics):
                    topic = topics[topic_idx]
                    report.append(self._format_topic_section(topic, stories_by_topic.get(topic_idx, []), is_continuing=True))
        
        return '\n'.join(report)
    
    def _format_topic_section(self, topic, stories, is_new=False, is_continuing=False):
        """æ ¼å¼åŒ–å•ä¸ªè¯é¢˜çš„æŠ¥å‘Šéƒ¨åˆ†"""
        # è·å–å…³é”®è¯
        keywords = topic.get('keywords', [])
        keyword_text = ', '.join([kw for kw, _ in keywords]) if keywords else "æ— å…³é”®è¯"
        
        topic_title = "ğŸ”¥ " if is_new else "ğŸ“Œ " if is_continuing else ""
        topic_title += f"**{keyword_text}**"
        
        section = [f"#### {topic_title}\n"]
        
        # æ·»åŠ æ•…äº‹åˆ—è¡¨
        if stories:
            # æŒ‰åˆ†æ•°æ’åº
            sorted_stories = sorted(stories, key=lambda x: x.get('score', 0), reverse=True)
            
            for i, story in enumerate(sorted_stories[:5]):  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                title = story.get('title', 'æ— æ ‡é¢˜')
                url = story.get('url', '')
                score = story.get('score', 0)
                
                if not url:
                    url = f"https://news.ycombinator.com/item?id={story.get('id', '')}"
                
                section.append(f"- [{title}]({url}) ({score} åˆ†)")
            
            # å¦‚æœæœ‰æ›´å¤šæ•…äº‹
            if len(sorted_stories) > 5:
                section.append(f"- *...è¿˜æœ‰ {len(sorted_stories) - 5} ä¸ªç›¸å…³æ•…äº‹*")
        else:
            section.append("*æ²¡æœ‰ç›¸å…³æ•…äº‹*")
        
        section.append("\n")
        return '\n'.join(section)

# å•å…ƒæµ‹è¯•å’Œç¤ºä¾‹ç”¨ä¾‹
if __name__ == "__main__":
    from src.clients.hacker_news_client import HackerNewsClient
    
    analyzer = HackerNewsTopicAnalyzer()
    client = HackerNewsClient()
    
    # è·å–å½“å‰çƒ­é—¨æ•…äº‹
    stories = client.get_top_stories_details(limit=30)
    
    if stories:
        # åˆ†æè¯é¢˜
        result = analyzer.analyze_topics(stories)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = analyzer.generate_report(result)
        print(report)
        
        # ä¿å­˜æŠ¥å‘Š
        date = datetime.now().strftime('%Y-%m-%d')
        hour = datetime.now().strftime('%H')
        os.makedirs(f"hacker_news/{date}", exist_ok=True)
        with open(f"hacker_news/{date}/{hour}_topics.md", 'w') as f:
            f.write(report)
    else:
        print("æ— æ³•è·å–Hacker Newsæ•…äº‹")

# å°†é™åˆ¶é™è‡³æ›´å°çš„å€¼
max_content_length = 5000  # æ›´æ¿€è¿›çš„æˆªæ–­ 